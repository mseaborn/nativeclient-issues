<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://schemas.google.com/g/2005" xmlns:ns2="http://schemas.google.com/projecthosting/issues/2009" ns1:etag="W/&quot;C0YFQ347eCl7ImA9WxFWFkQ.&quot;">
		<ns0:id>http://code.google.com/feeds/issues/p/nativeclient/issues/558/comments/full/9</ns0:id><ns0:author>
			<ns0:name>b...@google.com</ns0:name><ns0:uri>/u/105821748722532785568/</ns0:uri></ns0:author><ns0:content type="html">in this project, we've committed to making security the number one priority.  zero or 
negative performance overhead means little if the downloaded code can cause problems.  as 
a side effect, we're also committed to reducing attack surfaces whenever feasible (w/o 
increasing code complexity), and to push complexity into the untrusted code in order to 
keep the trusted code simple.  this means that if the linker has to be more complex in 
order to make the TCB better, then that's the preferred choice.  we also adhere to 
security engineering principles, such as reducing the TCB size, and whitelisting rather 
than blacklisting.

just because we don't reject elf binaries with data not described by program segment 
headers is not a reason to accept more program segment headers.  this kind of reasoning -
- well, perhaps only an extreme form -- leads us to implement things so that all 
(mis)features and potential vulnerabilities are not-quite-as-bad as the worse one that we 
know about (but cannot yet fix), so that there ends up being many fruits hanging at the 
same level, rather than there being a single (or a few) low hanging fruit(s).

now, if a nexe is rejected so it could never be run, we would never run any performance 
analysis tool on it.  we might run ncval -- if the reason for rejection is validation 
problems -- or similar tools when the reasons for its rejection was not sufficiently 
detailed.  i assert that rejecting nexes in the sel_ldr does reduce the probability that 
people would run (some) tools to analyze it.  especially if the tool is running outside 
of the NaCl environment, this could be a route by which malware is introduced, albeit 
only to programmers or security analysts, so not a huge target.  similarly, the debugger 
issue (and similarly, dynamic loaders) is really relevant only in the context of 
programmers trying to debug other people's code or to use other people's code, and is not 
a huge target compared to all users.

yes, being more picky often requires more code.  but not always!

the original whitelisting is not actually more code -- it's just what happens when a 
segment does not match the data table in the data-driven checker: we could reject the 
nexe (return an error), or we could ignore the segment (continue to the next segment).  
so, *especially* in this case where the engineering trade-off of having more checking 
code that could itself be wrong versus simpler but less picky code is minimal, our 
security sense pulls strongly in the direction of having a proper whitelist.</ns0:content><ns0:updated>2010-06-04T22:38:32.000Z</ns0:updated><ns0:published>2010-06-04T22:38:32.000Z</ns0:published><ns2:updates /><ns0:title>Comment 9 by b...@google.com</ns0:title><ns0:link href="http://code.google.com/p/nativeclient/issues/detail?id=558#c9" rel="alternate" type="text/html" /><ns0:link href="https://code.google.com/feeds/issues/p/nativeclient/issues/558/comments/full/9" rel="self" type="application/atom+xml" /></ns0:entry>
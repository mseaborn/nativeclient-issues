<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://schemas.google.com/g/2005" xmlns:ns2="http://schemas.google.com/projecthosting/issues/2009" ns1:etag="W/&quot;A0AMQH47eCl7ImA9Wx9TEEg.&quot;">
		<ns0:id>http://code.google.com/feeds/issues/p/nativeclient/issues/1173/comments/full/9</ns0:id><ns0:author>
			<ns0:name>abarth@chromium.org</ns0:name><ns0:uri>/u/abarth@chromium.org/</ns0:uri></ns0:author><ns0:content type="html">I think the issue is slightly more subtle than that.  It's not that we're assuming int is always 32 bits.  The assumption is given that int is 32 bits (which it happens to be in this platform) that int32_t is a typedef for int.</ns0:content><ns0:updated>2010-11-18T05:56:21.000Z</ns0:updated><ns0:published>2010-11-18T05:56:21.000Z</ns0:published><ns2:updates /><ns0:title>Comment 9 by abarth@chromium.org</ns0:title><ns0:link href="http://code.google.com/p/nativeclient/issues/detail?id=1173#c9" rel="alternate" type="text/html" /><ns0:link href="https://code.google.com/feeds/issues/p/nativeclient/issues/1173/comments/full/9" rel="self" type="application/atom+xml" /></ns0:entry>
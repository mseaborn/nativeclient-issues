<ns0:entry xmlns:ns0="http://www.w3.org/2005/Atom" xmlns:ns1="http://schemas.google.com/g/2005" xmlns:ns2="http://schemas.google.com/projecthosting/issues/2009" ns1:etag="W/&quot;D0MMR347eCl7ImA9WhVVEEU.&quot;">
		<ns0:id>http://code.google.com/feeds/issues/p/nativeclient/issues/2747/comments/full/13</ns0:id><ns0:author>
			<ns0:name>jvo...@google.com</ns0:name><ns0:uri>/u/102292187274959663599/</ns0:uri></ns0:author><ns0:content type="html">From:

glibc/libio/oldstdfiles.c

It looks like the app can either define the weak symbol "_IO_stdin_used" and use the newer stdio, or not define it and use the old stdio.

With the PSO stubs this was getting optimized out (it was in the "pre_opt" bitcode).  When linking against the ELF files during bitcode linking it would not optimize it out.

For comparison, with nacl-gcc, both x86-32 and x86-64 define it.  It does appear that you can preserve the symbol by adding it to the list of EXTERNs in crt1.x (and using PSO stubs).  However, the bitcode doesn't appear any different, so I'm not sure why "opt" would know to preserve it in one case but not the other.


===

Anyway, there is a workaround, but we still need to investigate run_thread_test for 32bit.</ns0:content><ns0:updated>2012-05-03T22:51:26.000Z</ns0:updated><ns0:published>2012-05-03T22:51:26.000Z</ns0:published><ns2:updates /><ns0:title>Comment 13 by jvo...@google.com</ns0:title><ns0:link href="http://code.google.com/p/nativeclient/issues/detail?id=2747#c13" rel="alternate" type="text/html" /><ns0:link href="https://code.google.com/feeds/issues/p/nativeclient/issues/2747/comments/full/13" rel="self" type="application/atom+xml" /></ns0:entry>